{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Julia: Flux.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://fluxml.ai/logo.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"flux.png\" width=900>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web page: https://fluxml.ai/\n",
    "\n",
    "Examples: [Model zoo](https://github.com/FluxML/model-zoo/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A single neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "import Flux.Tracker: gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model (generic function with 2 methods)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(W,b,x) = σ.(W * x + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " 0.4944243981290406 \n",
       " 0.366416169296387  \n",
       " 0.28300735323982606\n",
       " 0.2613408585166095 \n",
       " 0.22797402563434477"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single neuron 5 in 1 out\n",
    "W = randn(1, 5) # weights\n",
    "b = zeros(1)    # biases\n",
    "x = rand(5)     # input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Float64,1}:\n",
       " 0.3879253475178959"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(W, b, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(W, b, x) = Flux.mse(model(W,b,x), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012560727728984398"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(W,b,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-0.0263142 -0.0195013 … -0.013909 -0.0121332] (tracked), [-0.0532218] (tracked), [-0.00605705, 0.0602944, -0.0399467, 0.0576423, 0.00620331] (tracked))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(loss, W, b, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there can be hundreds of parameters in a neural network, we use a slightly different approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0461669 0.0725071 … 0.0186868 0.014291], [0.0751374])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux.Tracker: param, back!, grad\n",
    "\n",
    "W = param(randn(1, 5))\n",
    "b = param(zeros(1))\n",
    "x = rand(5)\n",
    "\n",
    "y = loss(W, b, x)\n",
    "\n",
    "back!(y) # Automatic differentiation (backpropagation)\n",
    "\n",
    "grad(W), grad(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use these gradients to update our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux.Tracker: update!\n",
    "\n",
    "η = 0.1\n",
    "for p in (W, b)\n",
    "  update!(p, -η * grad(p)) # gradient descent\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, Flux offers more sophisticated optimizers, like [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our full deep learning code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Chain(\n",
    "    Dense(10, 5),\n",
    "    Dense(5, 2),\n",
    "    softmax # normalize output neurons\n",
    ")\n",
    "\n",
    "opt = ADAM(0.01)\n",
    "\n",
    "data, labels = rand(10, 100), fill(0.5, 2, 100)\n",
    "\n",
    "loss(x, y) = sum(Flux.mse(m(x), y))\n",
    "\n",
    "Flux.train!(loss, params(m), [(data,labels)], opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
