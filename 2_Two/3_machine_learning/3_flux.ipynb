{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Julia: Flux.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://fluxml.ai/logo.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"flux.png\" width=900>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web page: https://fluxml.ai/\n",
    "\n",
    "Examples: [Model zoo](https://github.com/FluxML/model-zoo/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A single neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file C:\\Users\\carsten\\.julia\\compiled\\v1.1\\Flux\\QdkVy.ji for Flux [587475ba-b771-5e3f-ad9e-33799f191a9c]\n",
      "└ @ Base loading.jl:1184\n"
     ]
    }
   ],
   "source": [
    "using Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(W,b,x) = σ.(W * x + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " 0.5160287138043114 \n",
       " 0.9829057492794211 \n",
       " 0.15298584819665506\n",
       " 0.775591839600053  \n",
       " 0.13052244878138985"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single neuron 5 in 1 out\n",
    "W = randn(1, 5) # weights\n",
    "b = zeros(1)    # biases\n",
    "x = rand(5)     # input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Float64,1}:\n",
       " 0.529861161413611"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(W, b, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(W, b, x) = Flux.mse(model(W,b,x), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0008916889609697274"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(W,b,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.00767713 0.014623 … 0.0115387 0.00194183] (tracked), [0.0148773] (tracked), [0.00716614, -0.00376108, 0.0140566, -0.00306655, 0.0153683] (tracked))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Flux.Tracker: gradient # AD\n",
    "\n",
    "gradient(loss, W, b, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there can be hundreds of parameters in a neural network, we use a slightly different approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0460407 0.0247984 … 0.0329554 0.0321991], [0.0473799])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux.Tracker: param, back!, grad\n",
    "\n",
    "W = param(randn(1, 5))\n",
    "b = param(zeros(1))\n",
    "x = rand(5)\n",
    "\n",
    "y = loss(W, b, x)\n",
    "\n",
    "back!(y) # Automatic differentiation (backpropagation)\n",
    "\n",
    "grad(W), grad(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use these gradients to update our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux.Tracker: update!\n",
    "\n",
    "η = 0.1\n",
    "for p in (W, b)\n",
    "  update!(p, -η * grad(p)) # gradient descent\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, Flux offers more sophisticated optimizers, like [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A small Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Chain(\n",
    "    Dense(10, 5),\n",
    "    Dense(5, 2),\n",
    "    softmax # normalize output neurons\n",
    ")\n",
    "\n",
    "opt = ADAM(0.01)\n",
    "\n",
    "data, labels = rand(10, 100), fill(0.5, 2, 100)\n",
    "\n",
    "loss(x, y) = sum(Flux.mse(m(x), y))\n",
    "\n",
    "Flux.train!(loss, params(m), [(data,labels)], opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tracked 2-element Array{Float32,1}:\n",
       " 0.58191997f0\n",
       " 0.41808006f0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(rand(10)) # trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning the Ising transition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Flux: crossentropy, onecold, onehotbatch, throttle, @epochs\n",
    "using Printf, Statistics, Random\n",
    "using Base.Iterators: repeated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base.Iterators.Take{Base.Iterators.Repeated{Tuple{Array{Float64,2},Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}}}}(Base.Iterators.Repeated{Tuple{Array{Float64,2},Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}}}(([0.317564 0.460574 … 0.986012 0.884535; 0.286774 0.631787 … 0.849444 0.637524; … ; 0.873912 0.302887 … 0.401733 0.693611; 0.050445 0.195238 … 0.64096 0.557009], Bool[false false … true true; true true … false false])), 10)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our (fake) Monte Carlo configurations\n",
    "confs_left = rand(64,4000)\n",
    "confs_right = rand(64,4000)\n",
    "\n",
    "# set up as training data\n",
    "neach = size(confs_left, 2)\n",
    "X = hcat(confs_left, confs_right)\n",
    "labels = vcat(fill(1, neach), fill(0, neach))\n",
    "Y = onehotbatch(labels, 0:1)\n",
    "dataset = repeated((X, Y), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network + Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Epoch 1\n",
      "└ @ Main C:\\Users\\carsten\\.julia\\packages\\Flux\\qXNjB\\src\\optimise\\train.jl:105\n",
      "┌ Info: Epoch 2\n",
      "└ @ Main C:\\Users\\carsten\\.julia\\packages\\Flux\\qXNjB\\src\\optimise\\train.jl:105\n",
      "┌ Info: Epoch 3\n",
      "└ @ Main C:\\Users\\carsten\\.julia\\packages\\Flux\\qXNjB\\src\\optimise\\train.jl:105\n",
      "┌ Info: Epoch 4\n",
      "└ @ Main C:\\Users\\carsten\\.julia\\packages\\Flux\\qXNjB\\src\\optimise\\train.jl:105\n",
      "┌ Info: Epoch 5\n",
      "└ @ Main C:\\Users\\carsten\\.julia\\packages\\Flux\\qXNjB\\src\\optimise\\train.jl:105\n",
      "┌ Info: Epoch 6\n",
      "└ @ Main C:\\Users\\carsten\\.julia\\packages\\Flux\\qXNjB\\src\\optimise\\train.jl:105\n",
      "┌ Info: Epoch 7\n",
      "└ @ Main C:\\Users\\carsten\\.julia\\packages\\Flux\\qXNjB\\src\\optimise\\train.jl:105\n",
      "┌ Info: Epoch 8\n",
      "└ @ Main C:\\Users\\carsten\\.julia\\packages\\Flux\\qXNjB\\src\\optimise\\train.jl:105\n",
      "┌ Info: Epoch 9\n",
      "└ @ Main C:\\Users\\carsten\\.julia\\packages\\Flux\\qXNjB\\src\\optimise\\train.jl:105\n",
      "┌ Info: Epoch 10\n",
      "└ @ Main C:\\Users\\carsten\\.julia\\packages\\Flux\\qXNjB\\src\\optimise\\train.jl:105\n"
     ]
    }
   ],
   "source": [
    "# create neural network with 10 hidden units and 2 output neurons\n",
    "m = Chain(\n",
    "  Dense(64, 10, relu),\n",
    "  Dense(10, 2),\n",
    "  softmax)\n",
    "\n",
    "# define cost-function\n",
    "loss(x, y) = crossentropy(m(x), y)\n",
    "accuracy(x, y) = mean(onecold(m(x)) .== onecold(y))\n",
    "\n",
    "opt = ADAM()\n",
    "\n",
    "println(\"-------- Training\")\n",
    "@epochs 10 Flux.train!(loss, params(m), dataset, opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
